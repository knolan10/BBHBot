{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trigger_utils.trigger_utils import get_kowalski_ztf_queue\n",
    "import yaml\n",
    "\n",
    "with open('../config/Credentials.yaml', 'r') as file:\n",
    "    credentials = yaml.safe_load(file)\n",
    "fritz_token = credentials['fritz_token']\n",
    "ztf_allocation = credentials['allocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'queue_names': ['Caltech_Prince_2025-05-01',\n",
       "   'Caltech_Prince_2025-05-03',\n",
       "   'EP_2025-04-30_12',\n",
       "   'EP_2025-04-30_14',\n",
       "   'EP_2025-04-30_15',\n",
       "   'EP_2025-04-30_16',\n",
       "   'EP_2025-04-30_5',\n",
       "   'EP_2025-04-30_6',\n",
       "   'EP_2025-04-30_8',\n",
       "   'EP_2025-04-30_9',\n",
       "   'ToO_S250319bu_BBHBot_2025-05-01 01:25:32.352',\n",
       "   'Twilight_2025-04-30_e',\n",
       "   'Twilight_2025-04-30_m',\n",
       "   'Twilight_2025-05-01_e',\n",
       "   'Twilight_2025-05-01_m',\n",
       "   'Twilight_2025-05-02_e',\n",
       "   'Twilight_2025-05-02_m',\n",
       "   'Twilight_2025-05-03_e',\n",
       "   'Twilight_2025-05-03_m',\n",
       "   'Twilight_2025-05-04_e',\n",
       "   'Twilight_2025-05-04_m',\n",
       "   'default',\n",
       "   'fallback',\n",
       "   'missed_obs']},\n",
       " 'version': '1.4.0+fritz.86bd000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the current queue\n",
    "\n",
    "current_ztf_queue = get_kowalski_ztf_queue(fritz_token, ztf_allocation)\n",
    "current_ztf_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already submitted to queue\n",
      "Total time: 2940, probability: 0.7988238394848364\n"
     ]
    }
   ],
   "source": [
    "# get plan stats\n",
    "\n",
    "from trigger_utils.trigger_utils import get_plan_stats\n",
    "\n",
    "# get a specific plan\n",
    "gcnevent_id=13342\n",
    "queuename=\"S250319bu_BBHBot_2025-04-03 01:18:21.260\"\n",
    "stats = get_plan_stats(gcnevent_id, queuename, fritz_token, mode='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtracking NUZTF\n",
    "\n",
    "# scanner.plot_coverage() prints 'In total, __% of the contour was observed at least once' etc\n",
    "# scanner comes from scanner = SkymapScanner(event=skymap_file_url,prob_thresshold=0.9,n_days=3)\n",
    "# SkymapScanner is in nuztf.skymap_scanner\n",
    "# plot_coverage uses the function plot_overlap_with_observations(first_det_window_days=self.n_days,fields=fields(list=None))\n",
    "# plot_overlap_with_observations is in base_scanner.py\n",
    "# plot_overlap_with_observations uses the function calculate_overlap_with_observations\n",
    "\n",
    "# functions to track down for calculate_overlap_with_observations:\n",
    "# get_obs_summary\n",
    "# get_nested_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import astropy.units as u\n",
    "from tqdm import tqdm\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNS class\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MNS:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(jds: [int], backend=\"best\") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Get a dataframe of the coverage for a list of JDs\n",
    "\n",
    "    Will use the cache if available, otherwise will query the depot, and lastly TAP\n",
    "\n",
    "    :param jds: JDs\n",
    "    :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\" or \"masterlog\"\n",
    "    :return: Coverage dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    assert backend in [\n",
    "        \"best\",\n",
    "        \"depot\",\n",
    "        \"tap\",\n",
    "        \"skyvision\",\n",
    "        \"masterlog\",\n",
    "    ], f\"Invalid backend '{backend}'\"\n",
    "\n",
    "    # Clear any logs flagged as partial/incomplete\n",
    "\n",
    "    cache_files = coverage_dir.glob(\"*.json\")\n",
    "    partial_logs = [x for x in cache_files if partial_flag in str(x)]\n",
    "\n",
    "    if len(partial_logs) > 0:\n",
    "        print(f\"Removing the following partial logs: {partial_logs}\")\n",
    "        for partial_log in partial_logs:\n",
    "            partial_log.unlink()\n",
    "\n",
    "    # Only write missing logs\n",
    "\n",
    "    covered_jds = []\n",
    "\n",
    "    if backend in [\"best\", \"depot\"]:\n",
    "        for jd in jds:\n",
    "            if jd not in covered_jds:\n",
    "                depot_path = coverage_depot_path(jd)\n",
    "                if depot_path.exists():\n",
    "                    df = pd.read_json(depot_path)\n",
    "                    if len(df) > 0:\n",
    "                        covered_jds.append(jd)\n",
    "\n",
    "        missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "        if len(missing_logs) > 0:\n",
    "            print(\n",
    "                f\"Some logs were missing from the cache. \"\n",
    "                f\"Querying for the following JDs in depot: {missing_logs}\"\n",
    "            )\n",
    "            write_coverage_depot(missing_logs)\n",
    "\n",
    "    if backend in [\"best\", \"skyvision\"]:\n",
    "        \n",
    "        for jd in jds:\n",
    "            if jd not in covered_jds:\n",
    "                skyvision_path = coverage_skyvision_path(jd)\n",
    "                if skyvision_path.exists():\n",
    "                    df = pd.read_json(coverage_skyvision_path(jd))\n",
    "                    if len(df) > 0:\n",
    "                        covered_jds.append(jd)\n",
    "\n",
    "        missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "        if len(missing_logs) > 0:\n",
    "            print(\n",
    "                f\"Some logs were still missing from the cache. \"\n",
    "                f\"Querying for the following JDs from skyvision: {missing_logs}\"\n",
    "            )\n",
    "            write_coverage_skyvision(missing_logs)\n",
    "\n",
    "    # Try TAP for missing logs\n",
    "\n",
    "    if backend in [\"best\", \"tap\"]:\n",
    "        for jd in jds:\n",
    "            if jd not in covered_jds:\n",
    "                tap_path = coverage_tap_path(jd)\n",
    "                if tap_path.exists():\n",
    "                    df = pd.read_json(tap_path)\n",
    "                    if len(df) > 0:\n",
    "                        covered_jds.append(jd)\n",
    "\n",
    "        missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "        if len(missing_logs) > 0:\n",
    "            print(\n",
    "                f\"Some logs were still missing from the cache. \"\n",
    "                f\"Querying for the following JDs from TAP: {missing_logs}\"\n",
    "            )\n",
    "            write_coverage_tap(missing_logs)\n",
    "\n",
    "    # Load logs from cache\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for jd in tqdm(jds):\n",
    "\n",
    "        res = None\n",
    "\n",
    "        if backend in [\"best\", \"depot\"]:\n",
    "            df = pd.read_json(coverage_depot_path(jd))\n",
    "            if len(df) > 0:\n",
    "                res = df\n",
    "\n",
    "        if backend in [\"best\", \"skyvision\"]:\n",
    "            if res is None:\n",
    "                df = pd.read_json(coverage_skyvision_path(jd))\n",
    "                if len(df) > 0:\n",
    "                    res = df\n",
    "\n",
    "        if backend in [\"best\", \"tap\"]:\n",
    "            if res is None:\n",
    "                df = pd.read_json(coverage_tap_path(jd))\n",
    "                if len(df) > 0:\n",
    "                    res = df\n",
    "\n",
    "        if res is not None:\n",
    "            results.append(res)\n",
    "\n",
    "    if results:\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "        return result_df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_summary_depot(t_min: Time, t_max: Time, backend=\"best\") -> MNS | None:\n",
    "    \"\"\"\n",
    "    Get observation summary from depot\n",
    "\n",
    "    :param t_min: Start time\n",
    "    :param t_max: End time\n",
    "    :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\"\n",
    "    :return: MNS object\n",
    "    \"\"\"\n",
    "\n",
    "    jds = np.arange(int(t_min.jd) - 0.5, int(t_max.jd) + 1.5)\n",
    "\n",
    "    res = get_coverage(jds, backend=backend)\n",
    "\n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "\n",
    "    res[\"date\"] = Time(res[\"obsjd\"].to_numpy(), format=\"jd\").isot\n",
    "\n",
    "    mns = MNS(df=res)\n",
    "\n",
    "    mns.data.query(f\"obsjd >= {t_min.jd} and obsjd <= {t_max.jd}\", inplace=True)\n",
    "\n",
    "    mns.data.reset_index(inplace=True)\n",
    "    mns.data.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "    return mns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_summary(\n",
    "    t_min,\n",
    "    t_max=None,\n",
    "    max_days: float = None,\n",
    "    backend=\"best\",\n",
    ") -> MNS | None:\n",
    "    \"\"\"\n",
    "    Get observation summary from IPAC depot\n",
    "\n",
    "    :param t_min: Start time\n",
    "    :param t_max: End time\n",
    "    :param max_days: Maximum number of days\n",
    "    :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\"\n",
    "    :return: MNS object\n",
    "    \"\"\"\n",
    "    now = Time.now()\n",
    "\n",
    "    if t_max and max_days:\n",
    "        raise ValueError(\"Choose either t_max or max_days, not both\")\n",
    "\n",
    "    if t_max is None:\n",
    "        if max_days is None:\n",
    "            t_max = now\n",
    "        else:\n",
    "            t_max = t_min + (max_days * u.day)\n",
    "\n",
    "    if t_max > now:\n",
    "        t_max = now\n",
    "\n",
    "    print(f\"Getting observation logs  using backend {backend}.\")\n",
    "    mns = get_obs_summary_depot(t_min=t_min, t_max=t_max, backend=backend)\n",
    "\n",
    "    if mns is not None:\n",
    "        print(f\"Found {len(set(mns.data))} observations in total.\")\n",
    "    else:\n",
    "        print(f\"Found no observations using backend {backend}.\")\n",
    "\n",
    "    return mns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to define in class: t_min, nside, pixel_nos\n",
    "# remove self.logger\n",
    "\n",
    "def calculate_overlap_with_observations(\n",
    "    self,\n",
    "    first_det_window_days: float = 3.0,\n",
    "    min_sep: float = 0.01,\n",
    "    backend: str = \"best\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the overlap of the skymap with observations\n",
    "\n",
    "    :param first_det_window_days: First detection window in days\n",
    "    :param min_sep: Minimum separation between detections in days\n",
    "    :param backend: Backend to use for coverage calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    mns = get_obs_summary(\n",
    "        t_min=self.t_min, max_days=first_det_window_days, backend=backend\n",
    "    )\n",
    "\n",
    "    if mns is None:\n",
    "        return None, None, None\n",
    "\n",
    "    data = mns.data.copy()\n",
    "\n",
    "    mask = data[\"status\"] == 0\n",
    "\n",
    "    print(\n",
    "        f\"Found {mask.sum()} successful observations in the depot, \"\n",
    "        f\"corresponding to {np.mean(mask)*100:.2f}% of the total.\"\n",
    "    )\n",
    "\n",
    "    pix_map = dict()\n",
    "    pix_obs_times = dict()\n",
    "\n",
    "    nested_pix = get_nested_pix(nside=self.nside, logger=self.logger)\n",
    "\n",
    "    for i, obs_time in enumerate(tqdm(list(set(data[\"obsjd\"])))):\n",
    "        obs = data[data[\"obsjd\"] == obs_time]\n",
    "\n",
    "        field = obs[\"field_id\"].iloc[0]\n",
    "\n",
    "        try:\n",
    "            flat_pix = nested_pix[int(field)]\n",
    "\n",
    "            mask = obs[\"status\"].astype(int) == 0\n",
    "            indices = obs[\"qid\"].values[mask]\n",
    "\n",
    "            for qid in indices:\n",
    "                pixels = flat_pix[int(qid)]\n",
    "\n",
    "                for p in pixels:\n",
    "                    if p not in pix_obs_times.keys():\n",
    "                        pix_obs_times[p] = [obs_time]\n",
    "                    else:\n",
    "                        pix_obs_times[p] += [obs_time]\n",
    "\n",
    "                    if p not in pix_map.keys():\n",
    "                        pix_map[p] = [field]\n",
    "                    else:\n",
    "                        pix_map[p] += [field]\n",
    "\n",
    "        except (KeyError, ValueError):\n",
    "            print(\n",
    "                f\"Field {field} not found in nested pix dict. \"\n",
    "                f\"This might be an engineering observation.\"\n",
    "            )\n",
    "\n",
    "    npix = hp.nside2npix(self.nside)\n",
    "\n",
    "    ras, decs = hp.pixelfunc.pix2ang(\n",
    "        self.nside, hp.nest2ring(self.nside, self.pixel_nos), lonlat=True\n",
    "    )\n",
    "\n",
    "    radecs = SkyCoord(ra=ras * u.deg, dec=decs * u.deg)\n",
    "\n",
    "    coverage_data = []\n",
    "\n",
    "    overlapping_fields = []\n",
    "    times = []\n",
    "\n",
    "    for i, p in enumerate(tqdm(hp.nest2ring(self.nside, self.pixel_nos))):\n",
    "        entry = {\n",
    "            \"pixel_no\": p,\n",
    "            \"prob\": self.map_probs[i],\n",
    "            \"gal_b\": radecs[i].galactic.b.deg,\n",
    "            \"in_plane\": abs(radecs[i].galactic.b.deg) < 10.0,\n",
    "            \"ra_deg\": ras[i],\n",
    "            \"dec_deg\": decs[i],\n",
    "        }\n",
    "\n",
    "        if p in pix_obs_times.keys():\n",
    "            obs = pix_obs_times[p]\n",
    "\n",
    "            entry[\"n_det_class\"] = 2 if max(obs) - min(obs) > min_sep else 1\n",
    "\n",
    "            entry[\"latency_days\"] = min(obs) - self.t_min.jd\n",
    "\n",
    "            overlapping_fields += pix_map[p]\n",
    "\n",
    "            times += list(obs)\n",
    "        else:\n",
    "            entry[\"n_det_class\"] = 0\n",
    "            entry[\"latency_days\"] = np.nan\n",
    "\n",
    "        coverage_data.append(entry)\n",
    "\n",
    "    coverage_df = pd.DataFrame(coverage_data)\n",
    "\n",
    "    overlapping_fields = sorted(list(set(overlapping_fields)))\n",
    "\n",
    "    _observations = data.query(\"obsjd in @times\").reset_index(drop=True)[\n",
    "        [\"obsjd\", \"exposure_time\", \"filter_id\"]\n",
    "    ]\n",
    "    bands = [self.fid_to_band(fid) for fid in _observations[\"filter_id\"].values]\n",
    "    _observations[\"band\"] = bands\n",
    "    _observations.drop(columns=[\"filter_id\"], inplace=True)\n",
    "    self.observations = _observations\n",
    "\n",
    "    self.logger.info(\"All observations:\")\n",
    "    self.logger.info(f\"\\n{self.observations}\")\n",
    "\n",
    "    try:\n",
    "        self.first_obs = Time(min(times), format=\"jd\")\n",
    "        self.first_obs.utc.format = \"isot\"\n",
    "        self.last_obs = Time(max(times), format=\"jd\")\n",
    "        self.last_obs.utc.format = \"isot\"\n",
    "\n",
    "    except ValueError:\n",
    "        err = (\n",
    "            f\"No observations of this event were found at any time between \"\n",
    "            f\"{self.t_min} and {self.t_min + first_det_window_days * u.day}. \"\n",
    "            f\"Coverage overlap is 0%!\"\n",
    "        )\n",
    "        self.logger.error(err)\n",
    "        raise ValueError(err)\n",
    "\n",
    "    self.logger.info(f\"Observations started at {self.first_obs.isot}\")\n",
    "\n",
    "    return (\n",
    "        coverage_df,\n",
    "        times,\n",
    "        overlapping_fields,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlap_with_observations(\n",
    "        self, first_det_window_days=None, min_sep=0.01, fields=None, backend=\"best\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function to plot the overlap of the field with observations.\n",
    "\n",
    "        :param first_det_window_days: Window of time in days to consider for the first detection.\n",
    "        :param min_sep: Minimum separation between observations to consider them as separate.\n",
    "        :param fields: Fields to consider.\n",
    "        :param backend: Backend to use for coverage calculation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        (\n",
    "            coverage_df,\n",
    "            times,\n",
    "            overlapping_fields,\n",
    "        ) = self.calculate_overlap_with_observations(\n",
    "            first_det_window_days=first_det_window_days,\n",
    "            min_sep=min_sep,\n",
    "            fields=fields,\n",
    "            backend=backend,\n",
    "        )\n",
    "\n",
    "        if coverage_df is None:\n",
    "            self.logger.warning(\"Not plotting overlap with observations.\")\n",
    "            return\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(projection=\"aitoff\")\n",
    "\n",
    "        self.overlap_fields = list(set(overlapping_fields))\n",
    "\n",
    "        overlap_mask = (coverage_df[\"n_det_class\"] == 2) & ~coverage_df[\"in_plane\"]\n",
    "        self.overlap_prob = coverage_df[overlap_mask][\"prob\"].sum() * 100.0\n",
    "\n",
    "        size = hp.max_pixrad(self.nside) ** 2 * 50.0\n",
    "\n",
    "        veto_pixels = coverage_df.where(coverage_df[\"n_det_class\"] == 0)\n",
    "\n",
    "        if len(veto_pixels) > 0:\n",
    "            plt.scatter(\n",
    "                np.radians(veto_pixels[\"ra_deg\"]),\n",
    "                np.radians(veto_pixels[\"dec_deg\"]),\n",
    "                color=\"red\",\n",
    "                s=size,\n",
    "            )\n",
    "\n",
    "        plane_pixels = coverage_df.where(\n",
    "            coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] > 0)\n",
    "        )\n",
    "\n",
    "        if len(plane_pixels) > 0:\n",
    "            plt.scatter(\n",
    "                np.radians(self.wrap_around_180(plane_pixels[\"ra_deg\"])),\n",
    "                np.radians(plane_pixels[\"dec_deg\"]),\n",
    "                color=\"green\",\n",
    "                s=size,\n",
    "            )\n",
    "\n",
    "        single_pixels = coverage_df.where(\n",
    "            ~coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] == 1)\n",
    "        )\n",
    "\n",
    "        if len(single_pixels) > 0:\n",
    "            plt.scatter(\n",
    "                np.radians(self.wrap_around_180(single_pixels[\"ra_deg\"])),\n",
    "                np.radians(single_pixels[\"dec_deg\"]),\n",
    "                c=single_pixels[\"prob\"],\n",
    "                vmin=0.0,\n",
    "                vmax=max(self.data[self.key]),\n",
    "                s=size,\n",
    "                cmap=\"gray\",\n",
    "            )\n",
    "\n",
    "        double_pixels = coverage_df.where(\n",
    "            ~coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] == 2)\n",
    "        )\n",
    "\n",
    "        if len(double_pixels) > 0:\n",
    "            plt.scatter(\n",
    "                np.radians(self.wrap_around_180(double_pixels[\"ra_deg\"])),\n",
    "                np.radians(double_pixels[\"dec_deg\"]),\n",
    "                c=double_pixels[\"prob\"],\n",
    "                vmin=0.0,\n",
    "                vmax=max(self.data[self.key]),\n",
    "                s=size,\n",
    "            )\n",
    "\n",
    "        red_patch = mpatches.Patch(color=\"red\", label=\"Not observed\")\n",
    "        gray_patch = mpatches.Patch(color=\"gray\", label=\"Observed once\")\n",
    "        violet_patch = mpatches.Patch(\n",
    "            color=\"green\", label=\"Observed Galactic Plane (|b|<10)\"\n",
    "        )\n",
    "        plt.legend(handles=[red_patch, gray_patch, violet_patch])\n",
    "\n",
    "        message = (\n",
    "            \"In total, {0:.1f} % of the contour was observed at least once.\\n\"\n",
    "            \"This estimate includes {1:.1f} % of the contour \"\n",
    "            \"at a galactic latitude <10 deg.\\n\"\n",
    "            \"In total, {2:.1f} % of the contour was observed at least twice. \\n\"\n",
    "            \"In total, {3:.1f} % of the contour was observed at least twice, \"\n",
    "            \"and excluding low galactic latitudes.\\n\"\n",
    "            \"These estimates account for chip gaps.\".format(\n",
    "                100.0 * coverage_df.query(\"n_det_class > 0\")[\"prob\"].sum(),\n",
    "                100.0 * coverage_df.query(\"in_plane & n_det_class > 0\")[\"prob\"].sum(),\n",
    "                100.0 * coverage_df.query(\"n_det_class == 2\")[\"prob\"].sum(),\n",
    "                100.0 * coverage_df.query(\"n_det_class == 2 & ~in_plane\")[\"prob\"].sum(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        n_pixels = len(coverage_df.query(\"n_det_class > 0 & prob > 0.0\"))\n",
    "        n_double = len(coverage_df.query(\"n_det_class == 2 & prob > 0.0\"))\n",
    "        n_plane = len(coverage_df.query(\"in_plane & n_det_class > 0 & prob > 0.0\"))\n",
    "\n",
    "        self.healpix_area = self.pixel_area * n_pixels\n",
    "        self.double_extragalactic_area = self.pixel_area * n_double\n",
    "        plane_area = self.pixel_area * n_plane\n",
    "\n",
    "        self.overlap_fields = overlapping_fields\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{n_pixels} pixels were covered, covering approximately \"\n",
    "            f\"{self.healpix_area:.2g} sq deg.\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"{n_double} pixels were covered at least twice (b>10), \"\n",
    "            f\"covering approximately {self.double_extragalactic_area:.2g} sq deg.\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"{n_plane} pixels were covered at low galactic latitude, \"\n",
    "            f\"covering approximately {plane_area:.2g} sq deg.\"\n",
    "        )\n",
    "        return fig, message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
