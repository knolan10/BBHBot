{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trigger_utils.trigger_utils import query_kowalski_ztf_queue\n",
    "import yaml\n",
    "\n",
    "with open(\"../config/Credentials.yaml\", \"r\") as file:\n",
    "    credentials = yaml.safe_load(file)\n",
    "fritz_token = credentials[\"fritz_token\"]\n",
    "ztf_allocation = credentials[\"allocation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check ZTF queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Caltech_Prince_2025-06-18',\n",
       " 'Caltech_Prince_2025-06-20',\n",
       " 'Caltech_Prince_2025-06-22',\n",
       " 'Caltech_Prince_2025-06-24',\n",
       " 'Caltech_Prince_2025-06-26',\n",
       " 'Caltech_Prince_2025-06-28',\n",
       " 'Caltech_Prince_2025-06-30',\n",
       " 'Caltech_Prince_2025-07-02',\n",
       " 'EP_2025-06-18_11',\n",
       " 'EP_2025-06-18_12',\n",
       " 'EP_2025-06-18_14',\n",
       " 'Partnership_Plane_2025-06-19',\n",
       " 'Partnership_Plane_2025-06-21',\n",
       " 'Partnership_Plane_2025-06-23',\n",
       " 'Partnership_Plane_2025-06-25',\n",
       " 'Partnership_Plane_2025-06-27',\n",
       " 'Partnership_Plane_2025-06-29',\n",
       " 'Partnership_Plane_2025-07-03',\n",
       " 'Twilight_2025-06-18_e',\n",
       " 'Twilight_2025-06-18_m',\n",
       " 'Twilight_2025-06-19_e',\n",
       " 'Twilight_2025-06-19_m',\n",
       " 'Twilight_2025-06-20_e',\n",
       " 'Twilight_2025-06-20_m',\n",
       " 'Twilight_2025-06-21_e',\n",
       " 'Twilight_2025-06-21_m',\n",
       " 'Twilight_2025-06-22_e',\n",
       " 'Twilight_2025-06-22_m',\n",
       " 'Twilight_2025-06-23_e',\n",
       " 'Twilight_2025-06-23_m',\n",
       " 'Twilight_2025-06-24_e',\n",
       " 'default',\n",
       " 'fallback',\n",
       " 'missed_obs']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the current queue\n",
    "\n",
    "current_ztf_queue = query_kowalski_ztf_queue(None, fritz_token, ztf_allocation)\n",
    "current_ztf_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already submitted to queue\n",
      "Total time: 2940, probability: 0.7988238394848364\n"
     ]
    }
   ],
   "source": [
    "# get plan stats\n",
    "\n",
    "from trigger_utils.trigger_utils import get_plan_stats\n",
    "\n",
    "# get a specific plan\n",
    "gcnevent_id = 13342\n",
    "queuename = \"S250319bu_BBHBot_2025-04-03 01:18:21.260\"\n",
    "stats = get_plan_stats(gcnevent_id, queuename, fritz_token, mode=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check ZTF executed observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtracking NUZTF\n",
    "\n",
    "# scanner.plot_coverage() prints 'In total, __% of the contour was observed at least once' etc\n",
    "# scanner comes from scanner = SkymapScanner(event=skymap_file_url,prob_thresshold=0.9,n_days=3)\n",
    "# SkymapScanner is in nuztf.skymap_scanner\n",
    "# plot_coverage uses the function plot_overlap_with_observations(first_det_window_days=self.n_days,fields=fields(list=None))\n",
    "# plot_overlap_with_observations is in base_scanner.py\n",
    "# plot_overlap_with_observations uses the function calculate_overlap_with_observations\n",
    "\n",
    "# functions to track down for calculate_overlap_with_observations:\n",
    "# get_obs_summary\n",
    "# get_nested_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# MNS class\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# class MNS:\n",
    "#     def __init__(self, df: pd.DataFrame):\n",
    "#         self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_coverage(jds: [int], backend=\"best\") -> pd.DataFrame | None:\n",
    "#     \"\"\"\n",
    "#     Get a dataframe of the coverage for a list of JDs\n",
    "\n",
    "#     Will use the cache if available, otherwise will query the depot, and lastly TAP\n",
    "\n",
    "#     :param jds: JDs\n",
    "#     :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\" or \"masterlog\"\n",
    "#     :return: Coverage dataframe\n",
    "#     \"\"\"\n",
    "\n",
    "#     assert backend in [\n",
    "#         \"best\",\n",
    "#         \"depot\",\n",
    "#         \"tap\",\n",
    "#         \"skyvision\",\n",
    "#         \"masterlog\",\n",
    "#     ], f\"Invalid backend '{backend}'\"\n",
    "\n",
    "#     # Clear any logs flagged as partial/incomplete\n",
    "\n",
    "#     cache_files = coverage_dir.glob(\"*.json\")\n",
    "#     partial_logs = [x for x in cache_files if partial_flag in str(x)]\n",
    "\n",
    "#     if len(partial_logs) > 0:\n",
    "#         print(f\"Removing the following partial logs: {partial_logs}\")\n",
    "#         for partial_log in partial_logs:\n",
    "#             partial_log.unlink()\n",
    "\n",
    "#     # Only write missing logs\n",
    "\n",
    "#     covered_jds = []\n",
    "\n",
    "#     if backend in [\"best\", \"depot\"]:\n",
    "#         for jd in jds:\n",
    "#             if jd not in covered_jds:\n",
    "#                 depot_path = coverage_depot_path(jd)\n",
    "#                 if depot_path.exists():\n",
    "#                     df = pd.read_json(depot_path)\n",
    "#                     if len(df) > 0:\n",
    "#                         covered_jds.append(jd)\n",
    "\n",
    "#         missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "#         if len(missing_logs) > 0:\n",
    "#             print(\n",
    "#                 f\"Some logs were missing from the cache. \"\n",
    "#                 f\"Querying for the following JDs in depot: {missing_logs}\"\n",
    "#             )\n",
    "#             write_coverage_depot(missing_logs)\n",
    "\n",
    "#     if backend in [\"best\", \"skyvision\"]:\n",
    "#         for jd in jds:\n",
    "#             if jd not in covered_jds:\n",
    "#                 skyvision_path = coverage_skyvision_path(jd)\n",
    "#                 if skyvision_path.exists():\n",
    "#                     df = pd.read_json(coverage_skyvision_path(jd))\n",
    "#                     if len(df) > 0:\n",
    "#                         covered_jds.append(jd)\n",
    "\n",
    "#         missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "#         if len(missing_logs) > 0:\n",
    "#             print(\n",
    "#                 f\"Some logs were still missing from the cache. \"\n",
    "#                 f\"Querying for the following JDs from skyvision: {missing_logs}\"\n",
    "#             )\n",
    "#             write_coverage_skyvision(missing_logs)\n",
    "\n",
    "#     # Try TAP for missing logs\n",
    "\n",
    "#     if backend in [\"best\", \"tap\"]:\n",
    "#         for jd in jds:\n",
    "#             if jd not in covered_jds:\n",
    "#                 tap_path = coverage_tap_path(jd)\n",
    "#                 if tap_path.exists():\n",
    "#                     df = pd.read_json(tap_path)\n",
    "#                     if len(df) > 0:\n",
    "#                         covered_jds.append(jd)\n",
    "\n",
    "#         missing_logs = sorted(set(jds) - set(covered_jds))\n",
    "\n",
    "#         if len(missing_logs) > 0:\n",
    "#             print(\n",
    "#                 f\"Some logs were still missing from the cache. \"\n",
    "#                 f\"Querying for the following JDs from TAP: {missing_logs}\"\n",
    "#             )\n",
    "#             write_coverage_tap(missing_logs)\n",
    "\n",
    "#     # Load logs from cache\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     for jd in tqdm(jds):\n",
    "#         res = None\n",
    "\n",
    "#         if backend in [\"best\", \"depot\"]:\n",
    "#             df = pd.read_json(coverage_depot_path(jd))\n",
    "#             if len(df) > 0:\n",
    "#                 res = df\n",
    "\n",
    "#         if backend in [\"best\", \"skyvision\"]:\n",
    "#             if res is None:\n",
    "#                 df = pd.read_json(coverage_skyvision_path(jd))\n",
    "#                 if len(df) > 0:\n",
    "#                     res = df\n",
    "\n",
    "#         if backend in [\"best\", \"tap\"]:\n",
    "#             if res is None:\n",
    "#                 df = pd.read_json(coverage_tap_path(jd))\n",
    "#                 if len(df) > 0:\n",
    "#                     res = df\n",
    "\n",
    "#         if res is not None:\n",
    "#             results.append(res)\n",
    "\n",
    "#     if results:\n",
    "#         result_df = pd.concat(results, ignore_index=True)\n",
    "#         return result_df\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_obs_summary_depot(t_min: Time, t_max: Time, backend=\"best\") -> MNS | None:\n",
    "#     \"\"\"\n",
    "#     Get observation summary from depot\n",
    "\n",
    "#     :param t_min: Start time\n",
    "#     :param t_max: End time\n",
    "#     :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\"\n",
    "#     :return: MNS object\n",
    "#     \"\"\"\n",
    "\n",
    "#     jds = np.arange(int(t_min.jd) - 0.5, int(t_max.jd) + 1.5)\n",
    "\n",
    "#     res = get_coverage(jds, backend=backend)\n",
    "\n",
    "#     if len(res) == 0:\n",
    "#         return None\n",
    "\n",
    "#     res[\"date\"] = Time(res[\"obsjd\"].to_numpy(), format=\"jd\").isot\n",
    "\n",
    "#     mns = MNS(df=res)\n",
    "\n",
    "#     mns.data.query(f\"obsjd >= {t_min.jd} and obsjd <= {t_max.jd}\", inplace=True)\n",
    "\n",
    "#     mns.data.reset_index(inplace=True)\n",
    "#     mns.data.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "#     return mns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_obs_summary(\n",
    "#     t_min,\n",
    "#     t_max=None,\n",
    "#     max_days: float = None,\n",
    "#     backend=\"best\",\n",
    "# ) -> MNS | None:\n",
    "#     \"\"\"\n",
    "#     Get observation summary from IPAC depot\n",
    "\n",
    "#     :param t_min: Start time\n",
    "#     :param t_max: End time\n",
    "#     :param max_days: Maximum number of days\n",
    "#     :param backend: \"best\" or \"depot\" or \"tap\" or \"skyvision\"\n",
    "#     :return: MNS object\n",
    "#     \"\"\"\n",
    "#     now = Time.now()\n",
    "\n",
    "#     if t_max and max_days:\n",
    "#         raise ValueError(\"Choose either t_max or max_days, not both\")\n",
    "\n",
    "#     if t_max is None:\n",
    "#         if max_days is None:\n",
    "#             t_max = now\n",
    "#         else:\n",
    "#             t_max = t_min + (max_days * u.day)\n",
    "\n",
    "#     if t_max > now:\n",
    "#         t_max = now\n",
    "\n",
    "#     print(f\"Getting observation logs  using backend {backend}.\")\n",
    "#     mns = get_obs_summary_depot(t_min=t_min, t_max=t_max, backend=backend)\n",
    "\n",
    "#     if mns is not None:\n",
    "#         print(f\"Found {len(set(mns.data))} observations in total.\")\n",
    "#     else:\n",
    "#         print(f\"Found no observations using backend {backend}.\")\n",
    "\n",
    "#     return mns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to define in class: t_min, nside, pixel_nos\n",
    "# # remove self.logger\n",
    "\n",
    "\n",
    "# def calculate_overlap_with_observations(\n",
    "#     self,\n",
    "#     first_det_window_days: float = 3.0,\n",
    "#     min_sep: float = 0.01,\n",
    "#     backend: str = \"best\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Calculate the overlap of the skymap with observations\n",
    "\n",
    "#     :param first_det_window_days: First detection window in days\n",
    "#     :param min_sep: Minimum separation between detections in days\n",
    "#     :param backend: Backend to use for coverage calculation\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "\n",
    "#     mns = get_obs_summary(\n",
    "#         t_min=self.t_min, max_days=first_det_window_days, backend=backend\n",
    "#     )\n",
    "\n",
    "#     if mns is None:\n",
    "#         return None, None, None\n",
    "\n",
    "#     data = mns.data.copy()\n",
    "\n",
    "#     mask = data[\"status\"] == 0\n",
    "\n",
    "#     print(\n",
    "#         f\"Found {mask.sum()} successful observations in the depot, \"\n",
    "#         f\"corresponding to {np.mean(mask) * 100:.2f}% of the total.\"\n",
    "#     )\n",
    "\n",
    "#     pix_map = dict()\n",
    "#     pix_obs_times = dict()\n",
    "\n",
    "#     nested_pix = get_nested_pix(nside=self.nside, logger=self.logger)\n",
    "\n",
    "#     for i, obs_time in enumerate(tqdm(list(set(data[\"obsjd\"])))):\n",
    "#         obs = data[data[\"obsjd\"] == obs_time]\n",
    "\n",
    "#         field = obs[\"field_id\"].iloc[0]\n",
    "\n",
    "#         try:\n",
    "#             flat_pix = nested_pix[int(field)]\n",
    "\n",
    "#             mask = obs[\"status\"].astype(int) == 0\n",
    "#             indices = obs[\"qid\"].values[mask]\n",
    "\n",
    "#             for qid in indices:\n",
    "#                 pixels = flat_pix[int(qid)]\n",
    "\n",
    "#                 for p in pixels:\n",
    "#                     if p not in pix_obs_times.keys():\n",
    "#                         pix_obs_times[p] = [obs_time]\n",
    "#                     else:\n",
    "#                         pix_obs_times[p] += [obs_time]\n",
    "\n",
    "#                     if p not in pix_map.keys():\n",
    "#                         pix_map[p] = [field]\n",
    "#                     else:\n",
    "#                         pix_map[p] += [field]\n",
    "\n",
    "#         except (KeyError, ValueError):\n",
    "#             print(\n",
    "#                 f\"Field {field} not found in nested pix dict. \"\n",
    "#                 f\"This might be an engineering observation.\"\n",
    "#             )\n",
    "\n",
    "#     npix = hp.nside2npix(self.nside)\n",
    "\n",
    "#     ras, decs = hp.pixelfunc.pix2ang(\n",
    "#         self.nside, hp.nest2ring(self.nside, self.pixel_nos), lonlat=True\n",
    "#     )\n",
    "\n",
    "#     radecs = SkyCoord(ra=ras * u.deg, dec=decs * u.deg)\n",
    "\n",
    "#     coverage_data = []\n",
    "\n",
    "#     overlapping_fields = []\n",
    "#     times = []\n",
    "\n",
    "#     for i, p in enumerate(tqdm(hp.nest2ring(self.nside, self.pixel_nos))):\n",
    "#         entry = {\n",
    "#             \"pixel_no\": p,\n",
    "#             \"prob\": self.map_probs[i],\n",
    "#             \"gal_b\": radecs[i].galactic.b.deg,\n",
    "#             \"in_plane\": abs(radecs[i].galactic.b.deg) < 10.0,\n",
    "#             \"ra_deg\": ras[i],\n",
    "#             \"dec_deg\": decs[i],\n",
    "#         }\n",
    "\n",
    "#         if p in pix_obs_times.keys():\n",
    "#             obs = pix_obs_times[p]\n",
    "\n",
    "#             entry[\"n_det_class\"] = 2 if max(obs) - min(obs) > min_sep else 1\n",
    "\n",
    "#             entry[\"latency_days\"] = min(obs) - self.t_min.jd\n",
    "\n",
    "#             overlapping_fields += pix_map[p]\n",
    "\n",
    "#             times += list(obs)\n",
    "#         else:\n",
    "#             entry[\"n_det_class\"] = 0\n",
    "#             entry[\"latency_days\"] = np.nan\n",
    "\n",
    "#         coverage_data.append(entry)\n",
    "\n",
    "#     coverage_df = pd.DataFrame(coverage_data)\n",
    "\n",
    "#     overlapping_fields = sorted(list(set(overlapping_fields)))\n",
    "\n",
    "#     _observations = data.query(\"obsjd in @times\").reset_index(drop=True)[\n",
    "#         [\"obsjd\", \"exposure_time\", \"filter_id\"]\n",
    "#     ]\n",
    "#     bands = [self.fid_to_band(fid) for fid in _observations[\"filter_id\"].values]\n",
    "#     _observations[\"band\"] = bands\n",
    "#     _observations.drop(columns=[\"filter_id\"], inplace=True)\n",
    "#     self.observations = _observations\n",
    "\n",
    "#     self.logger.info(\"All observations:\")\n",
    "#     self.logger.info(f\"\\n{self.observations}\")\n",
    "\n",
    "#     try:\n",
    "#         self.first_obs = Time(min(times), format=\"jd\")\n",
    "#         self.first_obs.utc.format = \"isot\"\n",
    "#         self.last_obs = Time(max(times), format=\"jd\")\n",
    "#         self.last_obs.utc.format = \"isot\"\n",
    "\n",
    "#     except ValueError:\n",
    "#         err = (\n",
    "#             f\"No observations of this event were found at any time between \"\n",
    "#             f\"{self.t_min} and {self.t_min + first_det_window_days * u.day}. \"\n",
    "#             f\"Coverage overlap is 0%!\"\n",
    "#         )\n",
    "#         self.logger.error(err)\n",
    "#         raise ValueError(err)\n",
    "\n",
    "#     self.logger.info(f\"Observations started at {self.first_obs.isot}\")\n",
    "\n",
    "#     return (\n",
    "#         coverage_df,\n",
    "#         times,\n",
    "#         overlapping_fields,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_overlap_with_observations(\n",
    "#     self, first_det_window_days=None, min_sep=0.01, fields=None, backend=\"best\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to plot the overlap of the field with observations.\n",
    "\n",
    "#     :param first_det_window_days: Window of time in days to consider for the first detection.\n",
    "#     :param min_sep: Minimum separation between observations to consider them as separate.\n",
    "#     :param fields: Fields to consider.\n",
    "#     :param backend: Backend to use for coverage calculation\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     (\n",
    "#         coverage_df,\n",
    "#         times,\n",
    "#         overlapping_fields,\n",
    "#     ) = self.calculate_overlap_with_observations(\n",
    "#         first_det_window_days=first_det_window_days,\n",
    "#         min_sep=min_sep,\n",
    "#         fields=fields,\n",
    "#         backend=backend,\n",
    "#     )\n",
    "\n",
    "#     if coverage_df is None:\n",
    "#         self.logger.warning(\"Not plotting overlap with observations.\")\n",
    "#         return\n",
    "\n",
    "#     fig = plt.figure()\n",
    "#     plt.subplot(projection=\"aitoff\")\n",
    "\n",
    "#     self.overlap_fields = list(set(overlapping_fields))\n",
    "\n",
    "#     overlap_mask = (coverage_df[\"n_det_class\"] == 2) & ~coverage_df[\"in_plane\"]\n",
    "#     self.overlap_prob = coverage_df[overlap_mask][\"prob\"].sum() * 100.0\n",
    "\n",
    "#     size = hp.max_pixrad(self.nside) ** 2 * 50.0\n",
    "\n",
    "#     veto_pixels = coverage_df.where(coverage_df[\"n_det_class\"] == 0)\n",
    "\n",
    "#     if len(veto_pixels) > 0:\n",
    "#         plt.scatter(\n",
    "#             np.radians(veto_pixels[\"ra_deg\"]),\n",
    "#             np.radians(veto_pixels[\"dec_deg\"]),\n",
    "#             color=\"red\",\n",
    "#             s=size,\n",
    "#         )\n",
    "\n",
    "#     plane_pixels = coverage_df.where(\n",
    "#         coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] > 0)\n",
    "#     )\n",
    "\n",
    "#     if len(plane_pixels) > 0:\n",
    "#         plt.scatter(\n",
    "#             np.radians(self.wrap_around_180(plane_pixels[\"ra_deg\"])),\n",
    "#             np.radians(plane_pixels[\"dec_deg\"]),\n",
    "#             color=\"green\",\n",
    "#             s=size,\n",
    "#         )\n",
    "\n",
    "#     single_pixels = coverage_df.where(\n",
    "#         ~coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] == 1)\n",
    "#     )\n",
    "\n",
    "#     if len(single_pixels) > 0:\n",
    "#         plt.scatter(\n",
    "#             np.radians(self.wrap_around_180(single_pixels[\"ra_deg\"])),\n",
    "#             np.radians(single_pixels[\"dec_deg\"]),\n",
    "#             c=single_pixels[\"prob\"],\n",
    "#             vmin=0.0,\n",
    "#             vmax=max(self.data[self.key]),\n",
    "#             s=size,\n",
    "#             cmap=\"gray\",\n",
    "#         )\n",
    "\n",
    "#     double_pixels = coverage_df.where(\n",
    "#         ~coverage_df[\"in_plane\"] & (coverage_df[\"n_det_class\"] == 2)\n",
    "#     )\n",
    "\n",
    "#     if len(double_pixels) > 0:\n",
    "#         plt.scatter(\n",
    "#             np.radians(self.wrap_around_180(double_pixels[\"ra_deg\"])),\n",
    "#             np.radians(double_pixels[\"dec_deg\"]),\n",
    "#             c=double_pixels[\"prob\"],\n",
    "#             vmin=0.0,\n",
    "#             vmax=max(self.data[self.key]),\n",
    "#             s=size,\n",
    "#         )\n",
    "\n",
    "#     red_patch = mpatches.Patch(color=\"red\", label=\"Not observed\")\n",
    "#     gray_patch = mpatches.Patch(color=\"gray\", label=\"Observed once\")\n",
    "#     violet_patch = mpatches.Patch(\n",
    "#         color=\"green\", label=\"Observed Galactic Plane (|b|<10)\"\n",
    "#     )\n",
    "#     plt.legend(handles=[red_patch, gray_patch, violet_patch])\n",
    "\n",
    "#     message = (\n",
    "#         \"In total, {0:.1f} % of the contour was observed at least once.\\n\"\n",
    "#         \"This estimate includes {1:.1f} % of the contour \"\n",
    "#         \"at a galactic latitude <10 deg.\\n\"\n",
    "#         \"In total, {2:.1f} % of the contour was observed at least twice. \\n\"\n",
    "#         \"In total, {3:.1f} % of the contour was observed at least twice, \"\n",
    "#         \"and excluding low galactic latitudes.\\n\"\n",
    "#         \"These estimates account for chip gaps.\".format(\n",
    "#             100.0 * coverage_df.query(\"n_det_class > 0\")[\"prob\"].sum(),\n",
    "#             100.0 * coverage_df.query(\"in_plane & n_det_class > 0\")[\"prob\"].sum(),\n",
    "#             100.0 * coverage_df.query(\"n_det_class == 2\")[\"prob\"].sum(),\n",
    "#             100.0 * coverage_df.query(\"n_det_class == 2 & ~in_plane\")[\"prob\"].sum(),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     n_pixels = len(coverage_df.query(\"n_det_class > 0 & prob > 0.0\"))\n",
    "#     n_double = len(coverage_df.query(\"n_det_class == 2 & prob > 0.0\"))\n",
    "#     n_plane = len(coverage_df.query(\"in_plane & n_det_class > 0 & prob > 0.0\"))\n",
    "\n",
    "#     self.healpix_area = self.pixel_area * n_pixels\n",
    "#     self.double_extragalactic_area = self.pixel_area * n_double\n",
    "#     plane_area = self.pixel_area * n_plane\n",
    "\n",
    "#     self.overlap_fields = overlapping_fields\n",
    "\n",
    "#     self.logger.info(\n",
    "#         f\"{n_pixels} pixels were covered, covering approximately \"\n",
    "#         f\"{self.healpix_area:.2g} sq deg.\"\n",
    "#     )\n",
    "#     self.logger.info(\n",
    "#         f\"{n_double} pixels were covered at least twice (b>10), \"\n",
    "#         f\"covering approximately {self.double_extragalactic_area:.2g} sq deg.\"\n",
    "#     )\n",
    "#     self.logger.info(\n",
    "#         f\"{n_plane} pixels were covered at low galactic latitude, \"\n",
    "#         f\"covering approximately {plane_area:.2g} sq deg.\"\n",
    "#     )\n",
    "#     return fig, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penquins import Kowalski\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "with open(\"../config/Credentials.yaml\", \"r\") as file:\n",
    "    credentials = yaml.safe_load(file)\n",
    "kowalski_username = credentials[\"kowalski_username\"]\n",
    "kowalski_password = credentials[\"kowalski_password\"]\n",
    "\n",
    "protocol = \"https\"\n",
    "host = \"kowalski.caltech.edu\"\n",
    "port = 443\n",
    "timeout = 10\n",
    "token = os.getenv(\"KOWALSKI_TOKEN\")\n",
    "username = kowalski_username\n",
    "password = kowalski_password\n",
    "\n",
    "k = Kowalski(\n",
    "    protocol=protocol,\n",
    "    host=host,\n",
    "    port=port,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    timeout=timeout,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.time import Time\n",
    "\n",
    "dateobs = \"2025-05-09 00:00:00.000\"\n",
    "jdstart = Time(dateobs, format=\"iso\", scale=\"utc\").jd\n",
    "jdend = jdstart + 2.0\n",
    "\n",
    "query = {\n",
    "    \"query_type\": \"find\",\n",
    "    \"query\": {\n",
    "        \"catalog\": \"ZTF_ops\",\n",
    "        \"filter\": {\n",
    "            \"jd_start\": {\"$gte\": jdstart},\n",
    "            \"jd_end\": {\"$lte\": jdend},\n",
    "            \"exp\": {\"$gte\": 30},\n",
    "            \"filter\": {\"$in\": [1, 2]},\n",
    "            \"qcomment\": {\n",
    "                \"$nin\": [\n",
    "                    \"missing_FCD\",\n",
    "                    \"reference_building_g\",\n",
    "                    \"reference_building_r\",\n",
    "                    \"reference_building_i\",\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "        \"projection\": {},\n",
    "    },\n",
    "}\n",
    "\n",
    "response = k.query(query=query, max_n_threads=12).get(\"default\").get(\"data\")\n",
    "fields = [x[\"field\"] for x in response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: function to print table of observations overlapping skymap given a time period\n",
    "# TODO: plot observations on GW skymap localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crossmatch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cones(path, cumprob):  # path or file-like object\n",
    "    # max_order = None\n",
    "    with fits.open(path) as hdul:\n",
    "        hdul[1].columns\n",
    "        data = hdul[1].data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dmitryduev/penquins/pull/21/files\n",
    "# https://github.com/Theodlz/penquins/blob/abb7e9c87f8df3061315b0ce59174536e19db529/penquins/penquins.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
